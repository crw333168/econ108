---
title: "Econ108-01"
output: html_document
date: '2022-09-28'
---

```{r}
trucks <- read.csv("pickup.csv")
browser <- read.csv("web-browsers.csv")
```

1) Frequentist uncertainty is characterized by the thought experiment: “If I were able to see a new sample of data generated by the same processes and scenarios as my current data, how would my estimates change?” This is also known as the thought experiment of repeated experiments.

Bayesian uncertainty is characterized by inferences about parameters and the hypotheses are updated as evidence accumulates.

I prefer the school of frequentist uncertainty because it relies on existing data and not inferences.

2) The Monte Carlo simulation can be useful for the study of frequentist uncertainty because, given that frequentist uncertainty is characterized by repeated experiments and that in a Monte Carlo simulation, we mimic the data generating process as many times as we can, the Monte Carlo simulation effectively brings the repeated experiments of frequentist uncertainty theory to reality.

3) In a Monte Carlo simulation, you mimic the data generating process as many times as you can. In the bootstrap method, you use resampling from your current sample to mimic the sampling distribution. The Monte Carlo simulation is useful for properly evaluating bias. The bootstrap method is also good for reducing bias, but can approximate sampling distributions and construct confidence intervals. Essentially the main difference is that in bootstrapping, you use the original initial sample while the Monte Carlo simulation is based in generating new data.

4)
a)
```{r}
par(mfrow=c(2,2)) 
plot(price ~ factor(make), data=trucks)
plot(miles ~ factor(make), data=trucks)
```

Despite the similarities in the syntax of the commands, the factor() function categorizes the makes of the trucks, grouping them together and facilitating the creation of the box plot.

Whereas in the scatterplot, the trucks are not grouped together using the factor function; R plots the points individually, creating the scatterplot that we see.

```{r}
par(mfrow=c(2,2))
plot(price ~ year, data=trucks)
plot(price ~ miles, data=trucks)
```


The col=make colors the dots in based on make, allowing the viewer to see the plots of price against year and miles per make of truck, arguably giving scatterplot the ability to display a greater amount of information at once.

```{r}
trucks$make = factor(trucks$make) 
plot(price ~ year, data=trucks, col=make) 
plot(price ~ year, data=trucks, col=make) 
legend("topleft", fill=1:3, legend=levels(trucks$make)) 
plot(price ~ miles, data=trucks, col=make) 
legend("topright", fill=1:3, legend=levels(trucks$make))
```

b)
This generates a box-plot of price for each year instead.
```{r}
par(mfrow=c(2,2))
plot(price ~ factor(year), data=trucks)
```

5)
a)
The following R code simulates a large population
```{r}
set.seed(888)
mu0 <- 1
stddev <- 0.5
N <- 50000000
allx <- exp(rnorm(N, mu0, stddev))
hist(allx, probability=TRUE, breaks=100, main=paste("Population histogram of size", N))
lines(density(allx), col="red")
mu <- mean(allx)
sigma2 <- var(allx)
```

b)
Changing the mean and standard deviation will show that the following equation is consistent: Yi = EeZi = e^(μ+(σ^2)/2)
```{r}
set.seed(888)
mu0 <- 3
stddev <- .25
N <- 50000000
allx <- exp(rnorm(N, mu0, stddev))
hist(allx, probability=TRUE, breaks=100, main=paste("Population histogram of size", N))
lines(density(allx), col="red")
mu <- mean(allx)
sigma2 <- var(allx)
```

6)
```{r}
n <- 1; xmeans <- c(); R <- 100000
for (r in 1:R) {
xsample <- allx[sample.int(N, n)]
xmeans <- c(xmeans, mean(xsample))
}
hist(xmeans, probability=TRUE, breaks=100, main=paste("histogram of sample mean of size",
n))
lines(density(xmeans), col="red")
expectedbarx <- mean(xmeans); varofbarx <- var(xmeans)
```

```{r}
n <- 2; xmeans <- c(); R <- 100000
for (r in 1:R) {
xsample <- allx[sample.int(N, n)]
xmeans <- c(xmeans, mean(xsample))
}
hist(xmeans, probability=TRUE, breaks=100, main=paste("histogram of sample mean of size",
n))
lines(density(xmeans), col="red")
expectedbarx <- mean(xmeans); varofbarx <- var(xmeans)
```

```{r}
n <- 5; xmeans <- c(); R <- 100000
for (r in 1:R) {
xsample <- allx[sample.int(N, n)]
xmeans <- c(xmeans, mean(xsample))
}
hist(xmeans, probability=TRUE, breaks=100, main=paste("histogram of sample mean of size",
n))
lines(density(xmeans), col="red")
expectedbarx <- mean(xmeans); varofbarx <- var(xmeans)
```

```{r}
n <- 10; xmeans <- c(); R <- 100000
for (r in 1:R) {
xsample <- allx[sample.int(N, n)]
xmeans <- c(xmeans, mean(xsample))
}
hist(xmeans, probability=TRUE, breaks=100, main=paste("histogram of sample mean of size",
n))
lines(density(xmeans), col="red")
expectedbarx <- mean(xmeans); varofbarx <- var(xmeans)
```

```{r}
n <- 50; xmeans <- c(); R <- 100000
for (r in 1:R) {
xsample <- allx[sample.int(N, n)]
xmeans <- c(xmeans, mean(xsample))
}
hist(xmeans, probability=TRUE, breaks=100, main=paste("histogram of sample mean of size",
n))
lines(density(xmeans), col="red")
expectedbarx <- mean(xmeans); varofbarx <- var(xmeans)
```

Central Limit Theorem states if you have a population with mean µ and standard deviation σ and take sufficiently large random samples from the population, the distribution of the sample means will be approximately normally distributed. With smaller sizes, the histogram becomes more long tail, skewing right. With larger sizes, the histogram conforms to Central Limit Theorem and becomes more normally distributed.

7)
a)
```{r}
B <- 10000; mub <- c();
for (b in 1:B) {
sampleb = sample.int(nrow(browser),
replace=TRUE);
mub <- c(mub, mean(browser$spend[sampleb]));
}
sd(mub): 80.64
```

b)The new bootstrap standard deviation estimate is not terribly different from the original bootstrap standard deviation estimate. The old estimate was 80.79163. The new estimate is 80.06002.

c)
old:
```{r}
hist(mub, probability=TRUE, main=paste("histogram of mub"))
```
new:
```{r}
hist(mub, probability=TRUE, main=paste("histogram of mub"))
```
The histogram shapes and skews are similar. The difference only lies in the scale of frequency, with the highest peaks of the new frequency reaching 600 while the old peaks reach 2500.

d)
Central Limit Theorem states if you have a population with mean µ and standard deviation σ and take sufficiently large random samples from the population, the distribution of the sample means will be approximately normally distributed. Because the sample sizes are still so large, even if one is a fraction of the other, both histograms are normally distributed. The new histogram’s frequency is limited due to the smaller

population size. The standard deviation does not change because it measures how variable the population itself is and that has not changed.
